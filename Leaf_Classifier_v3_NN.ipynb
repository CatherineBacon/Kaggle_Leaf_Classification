{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# import libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn import preprocessing\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Activation, Dropout, Convolution1D, Flatten\n",
    "from keras.callbacks import EarlyStopping\n",
    "from keras.layers.noise import GaussianNoise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   id                species   margin1   margin2   margin3   margin4  \\\n",
      "0   1            Acer_Opalus  0.007812  0.023438  0.023438  0.003906   \n",
      "1   2  Pterocarya_Stenoptera  0.005859  0.000000  0.031250  0.015625   \n",
      "2   3   Quercus_Hartwissiana  0.005859  0.009766  0.019531  0.007812   \n",
      "3   5        Tilia_Tomentosa  0.000000  0.003906  0.023438  0.005859   \n",
      "4   6     Quercus_Variabilis  0.005859  0.003906  0.048828  0.009766   \n",
      "\n",
      "    margin5   margin6   margin7  margin8    ...      texture55  texture56  \\\n",
      "0  0.011719  0.009766  0.027344      0.0    ...       0.007812   0.000000   \n",
      "1  0.025391  0.001953  0.019531      0.0    ...       0.000977   0.000000   \n",
      "2  0.003906  0.005859  0.068359      0.0    ...       0.154300   0.000000   \n",
      "3  0.021484  0.019531  0.023438      0.0    ...       0.000000   0.000977   \n",
      "4  0.013672  0.015625  0.005859      0.0    ...       0.096680   0.000000   \n",
      "\n",
      "   texture57  texture58  texture59  texture60  texture61  texture62  \\\n",
      "0   0.002930   0.002930   0.035156        0.0        0.0   0.004883   \n",
      "1   0.000000   0.000977   0.023438        0.0        0.0   0.000977   \n",
      "2   0.005859   0.000977   0.007812        0.0        0.0   0.000000   \n",
      "3   0.000000   0.000000   0.020508        0.0        0.0   0.017578   \n",
      "4   0.021484   0.000000   0.000000        0.0        0.0   0.000000   \n",
      "\n",
      "   texture63  texture64  \n",
      "0   0.000000   0.025391  \n",
      "1   0.039062   0.022461  \n",
      "2   0.020508   0.002930  \n",
      "3   0.000000   0.047852  \n",
      "4   0.000000   0.031250  \n",
      "\n",
      "[5 rows x 194 columns]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "LabelBinarizer(neg_label=0, pos_label=1, sparse_output=False)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# read in data\n",
    "datafile = pd.read_csv('train.csv')\n",
    "test = pd.read_csv('test.csv')\n",
    "\n",
    "print(datafile.head())\n",
    "\n",
    "Y_train = datafile['species']\n",
    "X_train = datafile.drop('species', axis=1)\n",
    "\n",
    "# not looking at actual images for first pass, so drop id column from both test and train\n",
    "X_train = X_train.drop('id', axis=1)\n",
    "X_test = test.drop('id', axis=1)\n",
    "\n",
    "# one hot encode species\n",
    "lb = preprocessing.LabelBinarizer()\n",
    "lb.fit(Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 891 samples, validate on 99 samples\n",
      "Epoch 1/100\n",
      "891/891 [==============================] - 1s - loss: 4.5661 - acc: 0.0348 - val_loss: 4.5056 - val_acc: 0.0303\n",
      "Epoch 2/100\n",
      "891/891 [==============================] - 1s - loss: 4.3463 - acc: 0.0774 - val_loss: 4.2057 - val_acc: 0.0707\n",
      "Epoch 3/100\n",
      "891/891 [==============================] - 1s - loss: 3.9274 - acc: 0.1111 - val_loss: 3.7654 - val_acc: 0.1010\n",
      "Epoch 4/100\n",
      "891/891 [==============================] - 1s - loss: 3.4521 - acc: 0.1627 - val_loss: 3.2398 - val_acc: 0.1818\n",
      "Epoch 5/100\n",
      "891/891 [==============================] - 1s - loss: 3.0256 - acc: 0.2402 - val_loss: 2.7758 - val_acc: 0.3030\n",
      "Epoch 6/100\n",
      "891/891 [==============================] - 1s - loss: 2.6743 - acc: 0.2963 - val_loss: 2.4412 - val_acc: 0.4343\n",
      "Epoch 7/100\n",
      "891/891 [==============================] - 1s - loss: 2.4022 - acc: 0.3434 - val_loss: 2.0807 - val_acc: 0.5051\n",
      "Epoch 8/100\n",
      "891/891 [==============================] - 1s - loss: 2.2508 - acc: 0.3625 - val_loss: 1.8325 - val_acc: 0.6364\n",
      "Epoch 9/100\n",
      "891/891 [==============================] - 1s - loss: 2.0405 - acc: 0.4209 - val_loss: 1.6907 - val_acc: 0.6263\n",
      "Epoch 10/100\n",
      "891/891 [==============================] - 1s - loss: 1.9332 - acc: 0.4411 - val_loss: 1.4999 - val_acc: 0.6768\n",
      "Epoch 11/100\n",
      "891/891 [==============================] - 1s - loss: 1.7828 - acc: 0.4837 - val_loss: 1.3468 - val_acc: 0.7374\n",
      "Epoch 12/100\n",
      "891/891 [==============================] - 1s - loss: 1.5868 - acc: 0.5354 - val_loss: 1.2034 - val_acc: 0.7576\n",
      "Epoch 13/100\n",
      "891/891 [==============================] - 1s - loss: 1.5060 - acc: 0.5320 - val_loss: 1.0532 - val_acc: 0.8081\n",
      "Epoch 14/100\n",
      "891/891 [==============================] - 1s - loss: 1.3547 - acc: 0.6105 - val_loss: 0.9539 - val_acc: 0.7677\n",
      "Epoch 15/100\n",
      "891/891 [==============================] - 1s - loss: 1.3398 - acc: 0.6004 - val_loss: 0.8924 - val_acc: 0.8182\n",
      "Epoch 16/100\n",
      "891/891 [==============================] - 1s - loss: 1.2667 - acc: 0.6150 - val_loss: 0.8462 - val_acc: 0.7778\n",
      "Epoch 17/100\n",
      "891/891 [==============================] - 1s - loss: 1.1764 - acc: 0.6285 - val_loss: 0.7989 - val_acc: 0.8081\n",
      "Epoch 18/100\n",
      "891/891 [==============================] - 1s - loss: 1.1491 - acc: 0.6532 - val_loss: 0.7102 - val_acc: 0.8687\n",
      "Epoch 19/100\n",
      "891/891 [==============================] - 1s - loss: 1.0507 - acc: 0.6723 - val_loss: 0.6816 - val_acc: 0.8384\n",
      "Epoch 20/100\n",
      "891/891 [==============================] - 1s - loss: 0.9578 - acc: 0.6970 - val_loss: 0.6060 - val_acc: 0.8485\n",
      "Epoch 21/100\n",
      "891/891 [==============================] - 1s - loss: 0.9382 - acc: 0.7228 - val_loss: 0.5717 - val_acc: 0.8586\n",
      "Epoch 22/100\n",
      "891/891 [==============================] - 1s - loss: 0.9298 - acc: 0.7059 - val_loss: 0.4805 - val_acc: 0.9192\n",
      "Epoch 23/100\n",
      "891/891 [==============================] - 1s - loss: 0.8831 - acc: 0.7172 - val_loss: 0.4779 - val_acc: 0.9091\n",
      "Epoch 24/100\n",
      "891/891 [==============================] - 2s - loss: 0.9107 - acc: 0.7059 - val_loss: 0.4719 - val_acc: 0.8889\n",
      "Epoch 25/100\n",
      "891/891 [==============================] - 2s - loss: 0.7962 - acc: 0.7419 - val_loss: 0.4449 - val_acc: 0.8990\n",
      "Epoch 26/100\n",
      "891/891 [==============================] - 2s - loss: 0.7520 - acc: 0.7531 - val_loss: 0.3906 - val_acc: 0.9192\n",
      "Epoch 27/100\n",
      "891/891 [==============================] - 2s - loss: 0.7639 - acc: 0.7508 - val_loss: 0.4549 - val_acc: 0.8586\n",
      "Epoch 28/100\n",
      "891/891 [==============================] - 2s - loss: 0.7695 - acc: 0.7677 - val_loss: 0.4080 - val_acc: 0.8990\n",
      "Epoch 29/100\n",
      "891/891 [==============================] - 2s - loss: 0.6526 - acc: 0.7890 - val_loss: 0.3820 - val_acc: 0.9192\n",
      "Epoch 30/100\n",
      "891/891 [==============================] - 2s - loss: 0.7034 - acc: 0.7699 - val_loss: 0.3373 - val_acc: 0.9394\n",
      "Epoch 31/100\n",
      "891/891 [==============================] - 2s - loss: 0.6628 - acc: 0.7957 - val_loss: 0.3017 - val_acc: 0.9495\n",
      "Epoch 32/100\n",
      "891/891 [==============================] - 2s - loss: 0.6209 - acc: 0.7924 - val_loss: 0.2746 - val_acc: 0.9596\n",
      "Epoch 33/100\n",
      "891/891 [==============================] - 2s - loss: 0.6401 - acc: 0.7856 - val_loss: 0.3326 - val_acc: 0.9192\n",
      "Epoch 34/100\n",
      "891/891 [==============================] - 2s - loss: 0.6441 - acc: 0.7901 - val_loss: 0.3238 - val_acc: 0.8990\n",
      "Epoch 35/100\n",
      "891/891 [==============================] - 2s - loss: 0.5956 - acc: 0.8126 - val_loss: 0.3451 - val_acc: 0.8990\n",
      "Epoch 36/100\n",
      "891/891 [==============================] - 2s - loss: 0.5985 - acc: 0.8025 - val_loss: 0.2598 - val_acc: 0.9495\n",
      "Epoch 37/100\n",
      "891/891 [==============================] - 2s - loss: 0.5575 - acc: 0.8137 - val_loss: 0.2852 - val_acc: 0.9394\n",
      "Epoch 38/100\n",
      "891/891 [==============================] - 2s - loss: 0.5288 - acc: 0.8103 - val_loss: 0.2641 - val_acc: 0.9394\n",
      "Epoch 39/100\n",
      "891/891 [==============================] - 2s - loss: 0.5175 - acc: 0.8272 - val_loss: 0.2350 - val_acc: 0.9596\n",
      "Epoch 40/100\n",
      "891/891 [==============================] - 2s - loss: 0.5275 - acc: 0.8316 - val_loss: 0.2576 - val_acc: 0.9293\n",
      "Epoch 41/100\n",
      "891/891 [==============================] - 1s - loss: 0.4756 - acc: 0.8406 - val_loss: 0.2659 - val_acc: 0.9495\n",
      "Epoch 42/100\n",
      "891/891 [==============================] - 1s - loss: 0.4732 - acc: 0.8406 - val_loss: 0.2249 - val_acc: 0.9394\n",
      "Epoch 43/100\n",
      "891/891 [==============================] - 1s - loss: 0.4703 - acc: 0.8451 - val_loss: 0.1766 - val_acc: 0.9697\n",
      "Epoch 44/100\n",
      "891/891 [==============================] - 2s - loss: 0.4596 - acc: 0.8474 - val_loss: 0.2337 - val_acc: 0.9495\n",
      "Epoch 45/100\n",
      "891/891 [==============================] - 2s - loss: 0.4858 - acc: 0.8474 - val_loss: 0.2189 - val_acc: 0.9394\n",
      "Epoch 46/100\n",
      "891/891 [==============================] - 1s - loss: 0.4636 - acc: 0.8440 - val_loss: 0.2101 - val_acc: 0.9495\n",
      "Epoch 47/100\n",
      "891/891 [==============================] - 2s - loss: 0.4656 - acc: 0.8462 - val_loss: 0.1907 - val_acc: 0.9596\n",
      "Epoch 48/100\n",
      "891/891 [==============================] - 2s - loss: 0.4244 - acc: 0.8575 - val_loss: 0.2098 - val_acc: 0.9596\n",
      "Epoch 49/100\n",
      "891/891 [==============================] - 2s - loss: 0.4267 - acc: 0.8653 - val_loss: 0.2077 - val_acc: 0.9495\n",
      "Epoch 50/100\n",
      "891/891 [==============================] - 2s - loss: 0.4327 - acc: 0.8485 - val_loss: 0.1848 - val_acc: 0.9596\n",
      "Epoch 51/100\n",
      "891/891 [==============================] - 2s - loss: 0.3934 - acc: 0.8698 - val_loss: 0.2074 - val_acc: 0.9293\n",
      "Epoch 52/100\n",
      "891/891 [==============================] - 2s - loss: 0.3761 - acc: 0.8810 - val_loss: 0.2065 - val_acc: 0.9394\n",
      "Epoch 53/100\n",
      "891/891 [==============================] - 2s - loss: 0.4202 - acc: 0.8631 - val_loss: 0.1992 - val_acc: 0.9394\n",
      "Epoch 54/100\n",
      "891/891 [==============================] - 2s - loss: 0.4289 - acc: 0.8631 - val_loss: 0.1858 - val_acc: 0.9394\n",
      "Epoch 55/100\n",
      "891/891 [==============================] - 2s - loss: 0.4016 - acc: 0.8788 - val_loss: 0.1771 - val_acc: 0.9596\n",
      "Epoch 56/100\n",
      "891/891 [==============================] - 2s - loss: 0.3724 - acc: 0.8765 - val_loss: 0.1429 - val_acc: 0.9697\n",
      "Epoch 57/100\n",
      "891/891 [==============================] - 2s - loss: 0.3798 - acc: 0.8642 - val_loss: 0.1649 - val_acc: 0.9596\n",
      "Epoch 58/100\n",
      "891/891 [==============================] - 2s - loss: 0.3506 - acc: 0.8956 - val_loss: 0.1979 - val_acc: 0.9596\n",
      "Epoch 59/100\n",
      "891/891 [==============================] - 2s - loss: 0.4026 - acc: 0.8608 - val_loss: 0.1760 - val_acc: 0.9394\n",
      "Epoch 60/100\n",
      "891/891 [==============================] - 2s - loss: 0.3457 - acc: 0.8788 - val_loss: 0.1867 - val_acc: 0.9495\n",
      "Epoch 61/100\n",
      "891/891 [==============================] - 2s - loss: 0.2841 - acc: 0.9046 - val_loss: 0.1211 - val_acc: 0.9596\n",
      "Epoch 62/100\n",
      "891/891 [==============================] - 2s - loss: 0.3540 - acc: 0.8721 - val_loss: 0.1541 - val_acc: 0.9596\n",
      "Epoch 63/100\n",
      "891/891 [==============================] - 2s - loss: 0.3130 - acc: 0.8900 - val_loss: 0.1609 - val_acc: 0.9596\n",
      "Epoch 64/100\n",
      "891/891 [==============================] - 2s - loss: 0.3395 - acc: 0.8810 - val_loss: 0.1299 - val_acc: 0.9697\n",
      "Epoch 65/100\n",
      "891/891 [==============================] - 2s - loss: 0.3075 - acc: 0.9012 - val_loss: 0.1289 - val_acc: 0.9697\n",
      "Epoch 66/100\n",
      "891/891 [==============================] - 2s - loss: 0.3099 - acc: 0.9012 - val_loss: 0.1423 - val_acc: 0.9596\n",
      "Epoch 67/100\n",
      "891/891 [==============================] - 2s - loss: 0.3026 - acc: 0.8979 - val_loss: 0.1328 - val_acc: 0.9596\n",
      "Epoch 68/100\n",
      "891/891 [==============================] - 2s - loss: 0.2985 - acc: 0.9001 - val_loss: 0.1263 - val_acc: 0.9697\n",
      "Epoch 69/100\n",
      "891/891 [==============================] - 2s - loss: 0.2810 - acc: 0.8979 - val_loss: 0.1319 - val_acc: 0.9697\n",
      "Epoch 70/100\n",
      "891/891 [==============================] - 2s - loss: 0.2571 - acc: 0.9113 - val_loss: 0.1144 - val_acc: 0.9495\n",
      "Epoch 71/100\n",
      "891/891 [==============================] - 2s - loss: 0.3175 - acc: 0.8866 - val_loss: 0.1019 - val_acc: 0.9798\n",
      "Epoch 72/100\n",
      "891/891 [==============================] - 2s - loss: 0.2805 - acc: 0.9214 - val_loss: 0.1549 - val_acc: 0.9596\n",
      "Epoch 73/100\n",
      "891/891 [==============================] - 2s - loss: 0.2512 - acc: 0.9113 - val_loss: 0.1152 - val_acc: 0.9798\n",
      "Epoch 74/100\n",
      "891/891 [==============================] - 2s - loss: 0.2775 - acc: 0.9147 - val_loss: 0.1825 - val_acc: 0.9293\n",
      "Epoch 75/100\n",
      "891/891 [==============================] - 2s - loss: 0.2554 - acc: 0.9068 - val_loss: 0.1749 - val_acc: 0.9596\n",
      "Epoch 76/100\n",
      "891/891 [==============================] - 2s - loss: 0.2765 - acc: 0.9057 - val_loss: 0.1369 - val_acc: 0.9394\n",
      "Epoch 77/100\n",
      "891/891 [==============================] - 2s - loss: 0.2494 - acc: 0.9102 - val_loss: 0.1107 - val_acc: 0.9697\n",
      "Epoch 78/100\n",
      "891/891 [==============================] - 2s - loss: 0.2241 - acc: 0.9315 - val_loss: 0.1016 - val_acc: 0.9798\n",
      "Epoch 79/100\n",
      "891/891 [==============================] - 2s - loss: 0.2612 - acc: 0.9102 - val_loss: 0.1107 - val_acc: 0.9798\n",
      "Epoch 80/100\n",
      "891/891 [==============================] - 2s - loss: 0.3178 - acc: 0.8878 - val_loss: 0.1294 - val_acc: 0.9697\n",
      "Epoch 81/100\n",
      "891/891 [==============================] - 2s - loss: 0.2382 - acc: 0.9226 - val_loss: 0.1193 - val_acc: 0.9596\n",
      "Epoch 82/100\n",
      "891/891 [==============================] - 2s - loss: 0.2381 - acc: 0.9304 - val_loss: 0.1335 - val_acc: 0.9495\n",
      "Epoch 83/100\n",
      "891/891 [==============================] - 2s - loss: 0.2593 - acc: 0.9214 - val_loss: 0.1452 - val_acc: 0.9697\n",
      "Epoch 84/100\n",
      "891/891 [==============================] - 2s - loss: 0.2490 - acc: 0.9035 - val_loss: 0.1212 - val_acc: 0.9495\n",
      "Epoch 85/100\n",
      "891/891 [==============================] - 2s - loss: 0.2301 - acc: 0.9125 - val_loss: 0.1226 - val_acc: 0.9697\n",
      "Epoch 86/100\n",
      "891/891 [==============================] - 2s - loss: 0.2690 - acc: 0.9057 - val_loss: 0.1264 - val_acc: 0.9798\n",
      "Epoch 87/100\n",
      "891/891 [==============================] - 2s - loss: 0.2773 - acc: 0.9012 - val_loss: 0.0880 - val_acc: 0.9798\n",
      "Epoch 88/100\n",
      "891/891 [==============================] - 2s - loss: 0.2329 - acc: 0.9293 - val_loss: 0.1331 - val_acc: 0.9697\n",
      "Epoch 89/100\n",
      "891/891 [==============================] - 2s - loss: 0.2374 - acc: 0.9192 - val_loss: 0.1095 - val_acc: 0.9697\n",
      "Epoch 90/100\n",
      "891/891 [==============================] - 2s - loss: 0.2393 - acc: 0.9169 - val_loss: 0.1144 - val_acc: 0.9697\n",
      "Epoch 91/100\n",
      "891/891 [==============================] - 2s - loss: 0.2400 - acc: 0.9181 - val_loss: 0.1029 - val_acc: 0.9697\n",
      "Epoch 92/100\n",
      "891/891 [==============================] - 2s - loss: 0.1975 - acc: 0.9394 - val_loss: 0.1428 - val_acc: 0.9596\n",
      "Epoch 93/100\n",
      "891/891 [==============================] - 2s - loss: 0.2190 - acc: 0.9214 - val_loss: 0.1372 - val_acc: 0.9495\n",
      "Epoch 94/100\n",
      "891/891 [==============================] - 2s - loss: 0.2572 - acc: 0.9181 - val_loss: 0.1087 - val_acc: 0.9697\n",
      "Epoch 95/100\n",
      "891/891 [==============================] - 2s - loss: 0.2076 - acc: 0.9327 - val_loss: 0.1037 - val_acc: 0.9697\n",
      "Epoch 96/100\n",
      "891/891 [==============================] - 2s - loss: 0.2166 - acc: 0.9327 - val_loss: 0.1052 - val_acc: 0.9697\n",
      "Epoch 97/100\n",
      "891/891 [==============================] - 2s - loss: 0.2049 - acc: 0.9248 - val_loss: 0.1168 - val_acc: 0.9596\n",
      "Epoch 98/100\n",
      "891/891 [==============================] - 2s - loss: 0.2074 - acc: 0.9270 - val_loss: 0.0999 - val_acc: 0.9798\n",
      "Epoch 99/100\n",
      "891/891 [==============================] - 2s - loss: 0.2422 - acc: 0.9181 - val_loss: 0.1383 - val_acc: 0.9495\n",
      "Epoch 100/100\n",
      "891/891 [==============================] - 2s - loss: 0.2257 - acc: 0.9248 - val_loss: 0.0826 - val_acc: 0.9697\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7ff40b34afd0>"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = Sequential([\n",
    "    Dropout(0.3, input_shape=(X_train.shape[1],)),\n",
    "    Dense(8000),\n",
    "    Activation('relu'),\n",
    "    Dropout(0.5),\n",
    "    Dense(100),\n",
    "    Activation('relu'),\n",
    "    Dropout(0.2),\n",
    "    Dense(lb.classes_.size),\n",
    "    Activation('softmax'),\n",
    "])\n",
    "\n",
    "model.compile(\n",
    "    optimizer='rmsprop',\n",
    "    loss='categorical_crossentropy',\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "\n",
    "model.fit(\n",
    "    X_train.as_matrix(), \n",
    "    lb.transform(Y_train), \n",
    "    validation_split=0.1,\n",
    "    nb_epoch=100\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "594/594 [==============================] - 0s     \n"
     ]
    }
   ],
   "source": [
    "predictions = model.predict_proba(X_test.as_matrix())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "labels = test['id']\n",
    "headers = lb.classes_.tolist()\n",
    "headers = ['id']+headers\n",
    "results = pd.DataFrame(data=predictions)\n",
    "labels = pd.DataFrame(labels)\n",
    "results = pd.concat([labels, results], axis=1)\n",
    "results.columns = headers\n",
    "results.to_csv('results.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
